{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, path\n",
    "from PIL import Image\n",
    "import IPython\n",
    "import base64\n",
    "import imageio\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from tensorflow.keras import Sequential, optimizers, initializers, layers\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.specs import ArraySpec\n",
    "from tf_agents.environments import suite_gym, TFPyEnvironment, py_environment\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.trajectories import trajectory, TimeStep\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "TRAIN = True\n",
    "\n",
    "# N_EPISODES = 1\n",
    "N_EPISODES = 8\n",
    "EPISODES_PER_LOG = 10\n",
    "EPISODES_PER_SAVE = 500\n",
    "MAX_STEPS_PER_EPISODE = 100000\n",
    "\n",
    "FRAME_SKIP = 4\n",
    "DISCOUNT_RATE = 0.99\n",
    "LEARNING_RATE = 1e-7\n",
    "MOMENTUM = 0.95\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.1\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_BUFFER_MAX_LENGTH = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region\n",
    "def epsilon(step):\n",
    "    return max(EPSILON_MIN, EPSILON * (1 - step / MAX_STEPS_PER_EPISODE))\n",
    "\n",
    "\n",
    "def next_frame(env, policy, append):\n",
    "    time_step = env.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = env.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step,\n",
    "                                      action_step,\n",
    "                                      next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    append(traj)\n",
    "\n",
    "    return time_step\n",
    "\n",
    "\n",
    "def embed_gif(gif_buffer):\n",
    "    tag = f\"<img src=\\\"data:image/gif;base64,{base64.b64encode(gif_buffer).decode()}\\\"/>\"\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "\n",
    "def run_episodes_and_create_video(policy, eval_tf_env, eval_py_env):\n",
    "    num_episodes = 3\n",
    "    frames = []\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = eval_tf_env.reset()\n",
    "        frames.append(eval_py_env.render())\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = eval_tf_env.step(action_step.action)\n",
    "            frames.append(eval_py_env.render())\n",
    "    gif_file = io.BytesIO()\n",
    "    imageio.mimsave(gif_file, frames, format='gif', fps=60 / FRAME_SKIP)\n",
    "    IPython.display.display(embed_gif(gif_file.getvalue()))\n",
    "# endregion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# by screen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the assualt environment (with image)\n",
    "# AtariPreprocessing downsamples to 84x84, turns greyscale, and implements frame skipping\n",
    "py_env = AtariPreprocessing(gym.make('Assault-v0'),\n",
    "                            frame_skip=FRAME_SKIP,\n",
    "                            terminal_on_life_loss=True,\n",
    "                            screen_size=84)\n",
    "train_env = TFPyEnvironment(suite_gym.wrap_env(py_env))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = QNetwork(train_env.observation_spec(),\n",
    "                 train_env.action_spec(),\n",
    "\n",
    "                 # The rescales the values to a range of [-1, 1]\n",
    "                 preprocessing_layers=layers.Rescaling(scale=1./127.5,\n",
    "                                                       offset=-1),\n",
    "                 conv_layer_params=[(32, (8, 8), (4, 4)),\n",
    "                                    (64, (4, 4), (2, 2)),\n",
    "                                    (64, (3, 3), (1, 1))],\n",
    "                 fc_layer_params=(512, 7),\n",
    "\n",
    "                 kernel_initializer=initializers.VarianceScaling(scale=2.0,\n",
    "                                                                 mode='fan_in',\n",
    "                                                                 distribution='truncated_normal'))\n",
    "\n",
    "agent = DqnAgent(train_env.time_step_spec(),\n",
    "                 train_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "\n",
    "                 epsilon_greedy=lambda: epsilon(\n",
    "                     agent.train_step_counter.numpy()),\n",
    "\n",
    "                 gamma=DISCOUNT_RATE,\n",
    "                 optimizer=optimizers.SGD(LEARNING_RATE,\n",
    "                                          MOMENTUM,\n",
    "                                          nesterov=True))\n",
    "\n",
    "agent.initialize()\n",
    "agent.train = common.function(agent.train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Oliwe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:383: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "# implement replay buffer\n",
    "replay_buffer = TFUniformReplayBuffer(agent.collect_data_spec,\n",
    "                                      batch_size=1,\n",
    "                                      max_length=REPLAY_BUFFER_MAX_LENGTH)\n",
    "\n",
    "next_frame(train_env,\n",
    "           RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec()),\n",
    "           replay_buffer.add_batch)\n",
    "\n",
    "experience_batches = iter(replay_buffer.as_dataset(num_steps=2,\n",
    "                                                   sample_batch_size=BATCH_SIZE,\n",
    "                                                   ).prefetch(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = common.Checkpointer(path.join(getcwd(), 'checkpoint', 'screen'),\n",
    "                                   max_to_keep=1, agent=agent,\n",
    "                                   replay_buffer=replay_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oliwe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\atari\\environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\Oliwe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Oliwe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Oliwe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1  Step 53\tReward - 0.00000\t(Avg10 - 0.00000)\tLoss - 0.64206\n",
      "Episode 11  Step 23\tReward - 0.00000\t(Avg10 - 0.00000)\tLoss - 0.64216\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "average_rewards = []\n",
    "for episode in range(N_EPISODES):\n",
    "    time_step = train_env.reset()\n",
    "\n",
    "    agent.train_step_counter.assign(0)\n",
    "    n_step = agent.train_step_counter.numpy()\n",
    "\n",
    "    episode_rewards = 0\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    while n_step <= MAX_STEPS_PER_EPISODE and not time_step.is_last():\n",
    "        if (DEBUG):\n",
    "            # render the environment\n",
    "            py_env.render()\n",
    "\n",
    "        time_step = next_frame(train_env,\n",
    "                               agent.collect_policy,\n",
    "                               replay_buffer.add_batch)\n",
    "\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience, _info = next(experience_batches)\n",
    "        train_loss = agent.train(experience).loss\n",
    "\n",
    "        episode_rewards += time_step.reward.numpy()[0]\n",
    "        n_step = agent.train_step_counter.numpy()\n",
    "\n",
    "    rewards.append(episode_rewards)\n",
    "\n",
    "    if episode % EPISODES_PER_LOG == 0:\n",
    "        average_rewards.append(sum(average_rewards[-10:]) / 10)\n",
    "        print(f\"\\rEpisode {episode + 1}  Step {n_step}\"\n",
    "              f\"\\tReward - {time_step.reward.numpy()[0]:.5f}\"\n",
    "              f\"\\t(Avg10 - {(average_rewards[-1]):.5f})\"\n",
    "              f\"\\tLoss - {train_loss:.5f}\"\n",
    "              #   f\"\\tProgress {((((episode) * MAX_STEPS) + n_step)  * 100/ (MAX_STEPS * N_EPISODES)) :.1f}%\"\n",
    "              )\n",
    "\n",
    "    if episode % EPISODES_PER_SAVE == 0:\n",
    "        checkpointer.save(global_step=n_step)\n",
    "\n",
    "\n",
    "py_env.close()\n",
    "train_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.05500000000000001, 10.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = range(0, N_EPISODES, EPISODES_PER_LOG)\n",
    "plt.plot(iterations, average_rewards)\n",
    "plt.ylabel('Average Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylim(top=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# by RAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the assualt environment (with image)\n",
    "# AtariPreprocessing downsamples to 84x84, turns greyscale, and implements frame skipping\n",
    "py_env = gym.make('Assault-ram-v0')\n",
    "train_env = TFPyEnvironment(suite_gym.wrap_env(py_env))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = QNetwork(train_env.observation_spec(),\n",
    "                 train_env.action_spec(),\n",
    "\n",
    "                 # The rescales the values to a range of [-1, 1]\n",
    "                 preprocessing_layers=layers.Rescaling(scale=1./127.5,\n",
    "                                                       offset=-1),\n",
    "\n",
    "                 fc_layer_params=(32, 64, 64, 512, 7),\n",
    "\n",
    "                 kernel_initializer=initializers.VarianceScaling(scale=2.0,\n",
    "                                                                 mode='fan_in',\n",
    "                                                                 distribution='truncated_normal'))\n",
    "\n",
    "agent = DqnAgent(train_env.time_step_spec(),\n",
    "                 train_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "\n",
    "                 epsilon_greedy=lambda: epsilon(\n",
    "                     agent.train_step_counter.numpy()),\n",
    "\n",
    "                 gamma=DISCOUNT_RATE,\n",
    "                 optimizer=optimizers.SGD(LEARNING_RATE,\n",
    "                                          MOMENTUM,\n",
    "                                          nesterov=True))\n",
    "\n",
    "agent.initialize()\n",
    "agent.train = common.function(agent.train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement replay buffer\n",
    "replay_buffer = TFUniformReplayBuffer(agent.collect_data_spec,\n",
    "                                      batch_size=1,\n",
    "                                      max_length=REPLAY_BUFFER_MAX_LENGTH)\n",
    "\n",
    "next_frame(train_env,\n",
    "           RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec()),\n",
    "           replay_buffer.add_batch)\n",
    "\n",
    "experience_batches = iter(replay_buffer.as_dataset(num_steps=2,\n",
    "                                                   sample_batch_size=BATCH_SIZE,\n",
    "                                                   ).prefetch(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = common.Checkpointer(path.join(getcwd(), 'checkpoint', 'ram'),\n",
    "                                   max_to_keep=1, agent=agent,\n",
    "                                   replay_buffer=replay_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "average_rewards = []\n",
    "for episode in range(N_EPISODES):\n",
    "    time_step = train_env.reset()\n",
    "\n",
    "    agent.train_step_counter.assign(0)\n",
    "    n_step = agent.train_step_counter.numpy()\n",
    "\n",
    "    episode_rewards = 0\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    while n_step <= MAX_STEPS_PER_EPISODE and not time_step.is_last():\n",
    "        if (DEBUG):\n",
    "            # render the environment\n",
    "            py_env.render()\n",
    "\n",
    "        time_step = next_frame(train_env,\n",
    "                               agent.collect_policy,\n",
    "                               replay_buffer.add_batch)\n",
    "\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience, _info = next(experience_batches)\n",
    "        train_loss = agent.train(experience).loss\n",
    "\n",
    "        ram_bits.append(time_step.observation.numpy()[0])\n",
    "\n",
    "        episode_rewards += time_step.reward.numpy()[0]\n",
    "        n_step = agent.train_step_counter.numpy()\n",
    "\n",
    "    rewards.append(episode_rewards)\n",
    "\n",
    "    if episode % EPISODES_PER_LOG == 0:\n",
    "        average_rewards.append(sum(average_rewards[-10:]) / 10)\n",
    "        print(f\"\\rEpisode {episode + 1}  Step {n_step}\"\n",
    "              f\"\\tReward - {time_step.reward.numpy()[0]:.5f}\"\n",
    "              f\"\\t(Avg10 - {(average_rewards[-1]):.5f})\"\n",
    "              f\"\\tLoss - {train_loss:.5f}\"\n",
    "              #   f\"\\tProgress {((((episode) * MAX_STEPS) + n_step)  * 100/ (MAX_STEPS * N_EPISODES)) :.1f}%\"\n",
    "              )\n",
    "\n",
    "    if episode % EPISODES_PER_SAVE == 0:\n",
    "        checkpointer.save(global_step=n_step)\n",
    "\n",
    "\n",
    "py_env.close()\n",
    "train_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# by screen & RAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the assualt environment (with image)\n",
    "# AtariPreprocessing downsamples to 84x84, turns greyscale, and implements frame skipping\n",
    "py_env = AtariPreprocessing(gym.make('Assault-v0'),\n",
    "                            frame_skip=FRAME_SKIP,\n",
    "                            terminal_on_life_loss=True,\n",
    "                            screen_size=84)\n",
    "\n",
    "\n",
    "class CombinedEnv(py_environment.PyEnvironment):\n",
    "    def __init__(self, env):\n",
    "        super(CombinedEnv, self).__init__()\n",
    "        self.env = env\n",
    "\n",
    "    def _reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self.env.action_spec()\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return {\"screen\": self.env.observation_spec(),\n",
    "                \"ram\": ArraySpec(shape=(128,), dtype=np.float32)}\n",
    "\n",
    "    def _step(self, action):\n",
    "        time_step = self.env.step(action)\n",
    "        return time_step._replace(observation={\"screen\": self.env.observation_spec(),\n",
    "                                               \"ram\": py_env.env.ale.getRAM()})\n",
    "\n",
    "\n",
    "train_env = TFPyEnvironment(CombinedEnv(suite_gym.wrap_env(py_env)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_only_layers = Sequential([\n",
    "    layers.Rescaling(scale=1./127.5, offset=-1),\n",
    "    layers.Conv2D(32, (8, 8), (4, 4), activation='relu'),\n",
    "    layers.Conv2D(64, (4, 4), (2, 2), activation='relu'),\n",
    "    layers.Conv2D(64, (3, 3), (1, 1), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "])\n",
    "\n",
    "ram_only_layers = Sequential([\n",
    "    layers.Rescaling(scale=1./127.5, offset=-1),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu')\n",
    "])\n",
    "\n",
    "\n",
    "q_net = QNetwork(train_env.observation_spec(),\n",
    "                 train_env.action_spec(),\n",
    "\n",
    "                 preprocessing_layers={\"screen\": screen_only_layers,\n",
    "                                       \"ram\": ram_only_layers},\n",
    "                 preprocessing_combiner=layers.Concatenate(axis=-1),\n",
    "\n",
    "                 fc_layer_params=(512, 7),\n",
    "\n",
    "                 kernel_initializer=initializers.VarianceScaling(scale=2.0,\n",
    "                                                                 mode='fan_in',\n",
    "                                                                 distribution='truncated_normal'))\n",
    "\n",
    "agent = DqnAgent(train_env.time_step_spec(),\n",
    "                 train_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "\n",
    "                 epsilon_greedy=lambda: epsilon(\n",
    "                     agent.train_step_counter.numpy()),\n",
    "\n",
    "                 gamma=DISCOUNT_RATE,\n",
    "                 optimizer=optimizers.SGD(LEARNING_RATE,\n",
    "                                          MOMENTUM,\n",
    "                                          nesterov=True))\n",
    "\n",
    "agent.initialize()\n",
    "agent.train = common.function(agent.train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not pack sequence. Structure had 5 elements, but flat_sequence had 4 elements.  Structure: TimeStep(\n{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n 'observation': {'ram': TensorSpec(shape=(128,), dtype=tf.float32, name=None),\n                 'screen': BoundedTensorSpec(shape=(84, 84, 1), dtype=tf.uint8, name='observation', minimum=array(0, dtype=uint8), maximum=array(255, dtype=uint8))},\n 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}), flat_sequence: [<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, <tf.Tensor: shape=(1, 84, 84, 1), dtype=uint8, numpy=\narray([[[[0],\n         [0],\n         [0],\n         ...,\n         [0],\n         [0],\n         [0]],\n\n        [[0],\n         [0],\n         [0],\n         ...,\n         [0],\n         [0],\n         [0]],\n\n        [[0],\n         [0],\n         [0],\n         ...,\n         [0],\n         [0],\n         [0]],\n\n        ...,\n\n        [[0],\n         [0],\n         [0],\n         ...,\n         [0],\n         [0],\n         [0]],\n\n        [[0],\n         [0],\n         [0],\n         ...,\n         [0],\n         [0],\n         [0]],\n\n        [[0],\n         [0],\n         [0],\n         ...,\n         [0],\n         [0],\n         [0]]]], dtype=uint8)>].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[1;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[0;32m    631\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m     final_index, packed = _packed_nest_with_indices(structure, flat_sequence,\n\u001b[0m\u001b[0;32m    633\u001b[0m                                                     0, is_seq, sequence_fn)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m_packed_nest_with_indices\u001b[1;34m(structure, flat, index, is_seq, sequence_fn)\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m       new_index, child = _packed_nest_with_indices(s, flat, index, is_seq,\n\u001b[0m\u001b[0;32m    597\u001b[0m                                                    sequence_fn)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m_packed_nest_with_indices\u001b[1;34m(structure, flat, index, is_seq, sequence_fn)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m       \u001b[0mpacked\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m       \u001b[0mindex\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23120/3241418456.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtime_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_time_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# action_step = random_policy.action(time_step)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# next_time_step = train_env.step(action_step.action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\environments\\tf_environment.py\u001b[0m in \u001b[0;36mcurrent_time_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m           \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \"\"\"\n\u001b[1;32m--> 196\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_current_time_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step_dtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m           name='current_time_step_py_func')\n\u001b[1;32m--> 243\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step_from_numpy_function_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_time_step_from_numpy_function_outputs\u001b[1;34m(self, outputs)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[0mbatch_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatched\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[0mbatch_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m     time_step = _pack_named_sequence(outputs,\n\u001b[0m\u001b[0;32m    378\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_step_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                                      batch_shape)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001b[0m in \u001b[0;36m_pack_named_sequence\u001b[1;34m(flat_inputs, input_spec, batch_shape)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mnamed_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamed_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m   \u001b[0mnested_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamed_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mnested_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[1;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m   \"\"\"\n\u001b[1;32m--> 757\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[1;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[0mflat_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_structure\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m       raise ValueError(\n\u001b[0m\u001b[0;32m    640\u001b[0m           \u001b[1;34m\"Could not pack sequence. Structure had %d elements, but \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m           \u001b[1;34m\"flat_sequence had %d elements.  Structure: %s, flat_sequence: %s.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not pack sequence. Structure had 5 elements, but flat_sequence had 4 elements.  Structure: TimeStep(\n{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n 'observation': {'ram': TensorSpec(shape=(128,), dtype=tf.float32, name=None),\n                 'screen': BoundedTensorSpec(shape=(84, 84, 1), dtype=tf.uint8, name='observation', minimum=array(0, dtype=uint8), maximum=array(255, dtype=uint8))},\n 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')}), flat_sequence: [<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, <tf.Tensor: shape=(1, 84, 84, 1), dtype=uint8, numpy=\narray([[[[0],\n         [0],\n         [0],\n         ...,\n         [0],\n         [0],\n         [0]],\n\n        [[0],\n         [0],\n         [0],\n         ...,\n         [0],\n         [0],\n         [0]],\n\n        [[0],\n         [0],\n         [0],\n         ...,\n         [0],\n         [0],\n         [0]],\n\n        ...,\n\n        [[0],\n         [0],\n         [0],\n         ...,\n         [0],\n         [0],\n         [0]],\n\n        [[0],\n         [0],\n         [0],\n         ...,\n         [0],\n         [0],\n         [0]],\n\n        [[0],\n         [0],\n         [0],\n         ...,\n         [0],\n         [0],\n         [0]]]], dtype=uint8)>]."
     ]
    }
   ],
   "source": [
    "# implement replay buffer\n",
    "replay_buffer = TFUniformReplayBuffer(agent.collect_data_spec,\n",
    "                                      batch_size=1,\n",
    "                                      max_length=REPLAY_BUFFER_MAX_LENGTH)\n",
    "\n",
    "random_policy = RandomTFPolicy(train_env.time_step_spec(),\n",
    "                               train_env.action_spec())\n",
    "\n",
    "\n",
    "time_step = train_env.current_time_step()\n",
    "# action_step = random_policy.action(time_step)\n",
    "# next_time_step = train_env.step(action_step.action)\n",
    "# traj = trajectory.from_transition(time_step,\n",
    "#                                   action_step,\n",
    "#                                   next_time_step)\n",
    "# replay_buffer.add_batch(traj)\n",
    "\n",
    "\n",
    "# experience_batches = iter(replay_buffer.as_dataset(num_steps=2,\n",
    "#                                                    sample_batch_size=BATCH_SIZE,\n",
    "#                                                    ).prefetch(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = common.Checkpointer(path.join(getcwd(), 'checkpoint', 'ram_and_screen'),\n",
    "                                   max_to_keep=1, agent=agent,\n",
    "                                   replay_buffer=replay_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 84, 84, 1), dtype=uint8, numpy=\n",
      "array([[[[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0],\n",
      "         ...,\n",
      "         [0],\n",
      "         [0],\n",
      "         [0]]]], dtype=uint8)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
      " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "<tf_agents.networks.q_network.QNetwork object at 0x0000017980731430>: `inputs` and `input_tensor_spec` do not have matching structures:\n  .\nvs.\n  {'screen': ., 'ram': .}\nValues:\n  [[[[0]\n   [0]\n   [0]\n   ...\n   [0]\n   [0]\n   [0]]\n\n  [[0]\n   [0]\n   [0]\n   ...\n   [0]\n   [0]\n   [0]]\n\n  [[0]\n   [0]\n   [0]\n   ...\n   [0]\n   [0]\n   [0]]\n\n  ...\n\n  [[0]\n   [0]\n   [0]\n   ...\n   [0]\n   [0]\n   [0]]\n\n  [[0]\n   [0]\n   [0]\n   ...\n   [0]\n   [0]\n   [0]]\n\n  [[0]\n   [0]\n   [0]\n   ...\n   [0]\n   [0]\n   [0]]]]\nvs.\n  {'screen': BoundedTensorSpec(shape=(84, 84, 1), dtype=tf.uint8, name='observation', minimum=array(0, dtype=uint8), maximum=array(255, dtype=uint8)), 'ram': TensorSpec(shape=(128,), dtype=tf.float32, name=None)}.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23120/1588393395.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0maction_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mnext_time_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         traj = trajectory.from_transition(time_step,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\utils\\common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\policies\\epsilon_greedy_policy.py\u001b[0m in \u001b[0;36m_action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    114\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'epsilon_greedy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mgreedy_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_greedy_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[0mrandom_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_random_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\utils\\common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    558\u001b[0m     \"\"\"\n\u001b[0;32m    559\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tf_agents_tf_policy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m     \u001b[0mdistribution_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pytype: disable=wrong-arg-types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m     actions = tf.nest.map_structure(\n\u001b[0;32m    562\u001b[0m         \u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\policies\\greedy_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m     78\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mDeterministicWithLogProb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgreedy_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     distribution_step = self._wrapped_policy.distribution(\n\u001b[0m\u001b[0;32m     81\u001b[0m         time_step, policy_state)\n\u001b[0;32m     82\u001b[0m     return policy_step.PolicyStep(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36mdistribution\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memit_log_probability\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m       \u001b[1;31m# This here is set only for compatibility with info_spec in constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\policies\\q_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m    155\u001b[0m           network_observation)\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     q_values, policy_state = self._q_network(\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[0mnetwork_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         step_type=time_step.step_type)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\networks\\network.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \"\"\"\n\u001b[0;32m    389\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_tensor_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m       nest_utils.assert_matching_dtypes_and_inner_shapes(\n\u001b[0m\u001b[0;32m    391\u001b[0m           \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_tensor_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\utils\\nest_utils.py\u001b[0m in \u001b[0;36massert_matching_dtypes_and_inner_shapes\u001b[1;34m(tensors_or_specs, specs, caller, tensors_name, specs_name, allow_extra_fields)\u001b[0m\n\u001b[0;32m    367\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mallow_extra_fields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[0mtensors_or_specs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprune_extra_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors_or_specs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m   assert_same_structure(\n\u001b[0m\u001b[0;32m    370\u001b[0m       \u001b[0mtensors_or_specs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       \u001b[0mspecs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_agents\\utils\\nest_utils.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[1;34m(nest1, nest2, check_types, expand_composites, allow_shallow_nest1, message)\u001b[0m\n\u001b[0;32m    123\u001b[0m     str2 = tf.nest.map_structure(\n\u001b[0;32m    124\u001b[0m         lambda _: _DOT, nest2, expand_composites=expand_composites)\n\u001b[1;32m--> 125\u001b[1;33m     raise exception('{}:\\n  {}\\nvs.\\n  {}\\nValues:\\n  {}\\nvs.\\n  {}.'\n\u001b[0m\u001b[0;32m    126\u001b[0m                     .format(message, str1, str2, nest1, nest2))\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: <tf_agents.networks.q_network.QNetwork object at 0x0000017980731430>: `inputs` and `input_tensor_spec` do not have matching structures:\n  .\nvs.\n  {'screen': ., 'ram': .}\nValues:\n  [[[[0]\n   [0]\n   [0]\n   ...\n   [0]\n   [0]\n   [0]]\n\n  [[0]\n   [0]\n   [0]\n   ...\n   [0]\n   [0]\n   [0]]\n\n  [[0]\n   [0]\n   [0]\n   ...\n   [0]\n   [0]\n   [0]]\n\n  ...\n\n  [[0]\n   [0]\n   [0]\n   ...\n   [0]\n   [0]\n   [0]]\n\n  [[0]\n   [0]\n   [0]\n   ...\n   [0]\n   [0]\n   [0]]\n\n  [[0]\n   [0]\n   [0]\n   ...\n   [0]\n   [0]\n   [0]]]]\nvs.\n  {'screen': BoundedTensorSpec(shape=(84, 84, 1), dtype=tf.uint8, name='observation', minimum=array(0, dtype=uint8), maximum=array(255, dtype=uint8)), 'ram': TensorSpec(shape=(128,), dtype=tf.float32, name=None)}."
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "average_rewards = []\n",
    "for episode in range(N_EPISODES):\n",
    "    time_step = train_env.reset()\n",
    "\n",
    "    agent.train_step_counter.assign(0)\n",
    "    n_step = agent.train_step_counter.numpy()\n",
    "\n",
    "    episode_rewards = 0\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    while n_step <= MAX_STEPS_PER_EPISODE and not time_step.is_last():\n",
    "        if (DEBUG):\n",
    "            # render the environment\n",
    "            py_env.render()\n",
    "\n",
    "        time_step = train_env.current_time_step()\n",
    "        time_step._replace(observation={\"screen\": time_step,\n",
    "                                        \"ram\": py_env.env.ale.getRAM()})\n",
    "\n",
    "        action_step = agent.collect_policy.action({\"screen\": time_step,\n",
    "                                                   \"ram\": py_env.env.ale.getRAM()})\n",
    "        next_time_step = train_env.step(action_step.action)\n",
    "        traj = trajectory.from_transition(time_step,\n",
    "                                          action_step,\n",
    "                                          next_time_step)\n",
    "\n",
    "        # Add trajectory to the replay buffer\n",
    "        replay_buffer.add_batch(traj)\n",
    "\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience, _info = next(experience_batches)\n",
    "        train_loss = agent.train(experience).loss\n",
    "\n",
    "        episode_rewards += time_step.reward.numpy()[0]\n",
    "        n_step = agent.train_step_counter.numpy()\n",
    "\n",
    "    rewards.append(episode_rewards)\n",
    "\n",
    "    if episode % EPISODES_PER_LOG == 0:\n",
    "        average_rewards.append(sum(average_rewards[-10:]) / 10)\n",
    "        print(f\"\\rEpisode {episode + 1}  Step {n_step}\"\n",
    "              f\"\\tReward - {time_step.reward.numpy()[0]:.5f}\"\n",
    "              f\"\\t(Avg10 - {(average_rewards[-1]):.5f})\"\n",
    "              f\"\\tLoss - {train_loss:.5f}\"\n",
    "              #   f\"\\tProgress {((((episode) * MAX_STEPS) + n_step)  * 100/ (MAX_STEPS * N_EPISODES)) :.1f}%\"\n",
    "              )\n",
    "\n",
    "    if episode % EPISODES_PER_SAVE == 0:\n",
    "        checkpointer.save(global_step=n_step)\n",
    "\n",
    "\n",
    "py_env.close()\n",
    "train_env.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0ef11f269b7839d155d2f9779442432e0c58cfdafb9c7d7b1c8ec222ef79f09"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
