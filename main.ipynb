{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    def __init__(self, env):\n",
    "        self.action_size = env.action_space.n\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        return random.choice(range(self.action_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_network(X_state, name):\n",
    "    prev_layer = X_state / 128  # the values will be between [-1, 1]\n",
    "    initializer = tf.variance_scaling_initializer()\n",
    "    hidden_activation = tf.nn.relu\n",
    "\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        prev_layer = tf.layers.conv2d(prev_layer, filters=32,\n",
    "                                      kernel_size=8, strides=4,\n",
    "                                      padding=\"SAME\", activation=hidden_activation,\n",
    "                                      kernel_initializer=initializer)\n",
    "\n",
    "        prev_layer = tf.layers.conv2d(prev_layer, filters=64,\n",
    "                                      kernel_size=4, strides=2,\n",
    "                                      padding=\"SAME\", activation=hidden_activation,\n",
    "                                      kernel_initializer=initializer)\n",
    "\n",
    "        prev_layer = tf.layers.conv2d(prev_layer, filters=64,\n",
    "                                      kernel_size=3, strides=1,\n",
    "                                      padding=\"SAME\", activation=hidden_activation,\n",
    "                                      kernel_initializer=initializer)\n",
    "\n",
    "        last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, 64 * 12*10])\n",
    "\n",
    "        hidden = tf.layers.dense(last_conv_layer_flat, 512,\n",
    "                                 activation=hidden_activation, kernel_initializer=initializer)\n",
    "        outputs = tf.layers.dense(\n",
    "            hidden, env.action_space.n, kernel_initializer=initializer)\n",
    "\n",
    "    trainable_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)\n",
    "\n",
    "    trainable_vars_by_name = {\n",
    "        var.name[len(scope.name):]: var for var in trainable_vars}\n",
    "\n",
    "    return outputs, trainable_vars_by_name\n",
    "\n",
    "\n",
    "class QLearningAgent():\n",
    "    def __init__(self, env, learning_rate=0.001, momemtum=0.95):\n",
    "        self.action_size = env.action_space.n\n",
    "        self.loss_val = np.infty\n",
    "        tf.reset_default_graph()\n",
    "        tf.disable_eager_execution()\n",
    "\n",
    "        self.discount_rate = 0.99\n",
    "\n",
    "        self.checkpoint_path = \"./my_dqn_assualt.ckpt\"\n",
    "\n",
    "        self.X_state = tf.placeholder(tf.float32, shape=[None, 96, 80, 1])\n",
    "        self.online_q_values, self.online_vars = q_network(\n",
    "            self.X_state, name=\"q_networks/online\")\n",
    "        self.target_q_values, self.target_vars = q_network(\n",
    "            self.X_state, name=\"q_networks/target\")\n",
    "\n",
    "        # Define the operations to copy the online network to the target network\n",
    "        self.copy_ops = [target_var.assign(self.online_vars[var_name])\n",
    "                         for var_name, target_var in self.target_vars.items()]\n",
    "        self.copy_online_to_target = tf.group(*self.copy_ops)\n",
    "\n",
    "        # The structure of the training\n",
    "        with tf.variable_scope(\"train\"):\n",
    "            self.X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "            self.y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "            self.q_value = tf.reduce_sum(self.online_q_values * tf.one_hot(self.X_action, self.action_size),\n",
    "                                         axis=1, keepdims=True)\n",
    "\n",
    "            # A value between 0 and infty\n",
    "            self.error = tf.abs(self.y - self.q_value)\n",
    "            # If it is above 1 then it becomes 1\n",
    "            self.clipped_error = tf.clip_by_value(self.error, 0.0, 1.0)\n",
    "            self.linear_error = 2 * (self.error - self.clipped_error)\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.square(self.clipped_error) + self.linear_error)\n",
    "\n",
    "            self.global_step = tf.Variable(\n",
    "                0, trainable=False, name=\"global_step\")\n",
    "            self.optimizer = tf.train.MomentumOptimizer(\n",
    "                learning_rate, momentum=momemtum, use_nesterov=True)\n",
    "            self.training_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=self.global_step)\n",
    "\n",
    "        # Saving\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess = tf.Session()\n",
    "        if os.path.isfile(self.checkpoint_path + \".index\"):\n",
    "            self.saver.restore(self.sess, self.checkpoint_path)\n",
    "        else:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.sess.run(self.copy_online_to_target)\n",
    "\n",
    "    def get_action(self, q_values, step):\n",
    "        epsilon = max(0.1, 1 - (0.9/2000000) * step)\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def train(self, state_val, action_val, reward, next_state_val, continues):\n",
    "        next_q_values = self.target_q_values.eval(\n",
    "            feed_dict={self.X_state: np.array([next_state_val])})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        # We can now compute the target value\n",
    "        y_val = reward + continues * self.discount_rate * max_next_q_values\n",
    "        _, self.loss_val = self.sess.run([self.training_op, self.loss],\n",
    "                                         feed_dict={self.X_state: np.array([state_val]),\n",
    "                                                    self.X_action: np.array([action_val]),\n",
    "                                                    self.y: y_val})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# by screen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the assualt with image environment\n",
    "env = gym.make('Assault-v0', render_mode=None)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    # compress\n",
    "    img = obs[1:192:2, ::2]\n",
    "\n",
    "    # The values or between 0 and 255\n",
    "    # grayscale-ify\n",
    "    img = img.mean(axis=2)\n",
    "\n",
    "    # normalized the values between -128 and 127\n",
    "    img = (img - 128).astype(np.int8)\n",
    "    return img.reshape(96, 80, 1)\n",
    "\n",
    "\n",
    "state = preprocess_observation(obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oliwe\\AppData\\Local\\Temp/ipykernel_17464/2749429955.py:7: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  prev_layer = tf.layers.conv2d(prev_layer, filters=32,\n",
      "C:\\Users\\Oliwe\\AppData\\Local\\Temp/ipykernel_17464/2749429955.py:12: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  prev_layer = tf.layers.conv2d(prev_layer, filters=64,\n",
      "C:\\Users\\Oliwe\\AppData\\Local\\Temp/ipykernel_17464/2749429955.py:17: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  prev_layer = tf.layers.conv2d(prev_layer, filters=64,\n",
      "C:\\Users\\Oliwe\\AppData\\Local\\Temp/ipykernel_17464/2749429955.py:24: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  hidden = tf.layers.dense(last_conv_layer_flat, 512,\n",
      "C:\\Users\\Oliwe\\AppData\\Local\\Temp/ipykernel_17464/2749429955.py:26: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  outputs = tf.layers.dense(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_dqn_assualt.ckpt\n",
      "Training step 300/300 (100.0)% \t Loss 40.67892"
     ]
    }
   ],
   "source": [
    "agent = QLearningAgent(env)\n",
    "# copy_steps = 5000\n",
    "# save_steps = 1000\n",
    "\n",
    "with agent.sess:\n",
    "    step = 0\n",
    "    while (step < n_steps):\n",
    "        step = agent.global_step.eval()\n",
    "        print(f\"\\rTraining step {step}/{n_steps} ({step * 100 / n_steps:.1f})% \\t Loss {agent.loss_val:.5f}\",\n",
    "              end=\"\")\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        q_values = agent.online_q_values.eval(\n",
    "            feed_dict={agent.X_state: [state]})\n",
    "        action = agent.get_action(q_values, step)\n",
    "\n",
    "        # We play the action from the agent\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observation(next_obs)\n",
    "        agent.train(state, action, reward, next_state, 1.0 - done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # if step % copy_steps == 0:\n",
    "        #     agent.copy_online_to_target.run()\n",
    "\n",
    "        # if step % save_steps == 0:\n",
    "        #     agent.saver.save(agent.sess, agent.checkpoint_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode: 233.1\n"
     ]
    }
   ],
   "source": [
    "reward_episodes = []\n",
    "numberOfEpisodes = 10\n",
    "agent = RandomAgent(env)\n",
    "\n",
    "for ep in range(numberOfEpisodes):\n",
    "    current_obs = env.reset()\n",
    "    done = False\n",
    "    total_reward_ep = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(current_obs)\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        total_reward_ep += reward\n",
    "    reward_episodes.append(total_reward_ep)\n",
    "\n",
    "print(\"Average reward per episode: {}\".format(\n",
    "    np.sum(reward_episodes)/numberOfEpisodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the assualt with image environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# by RAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the assualt with ram environment\n",
    "# env = gym.make('Assault-ram-v0', render_mode=None)\n",
    "# obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the assualt with ram environment\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# by screen & RAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0ef11f269b7839d155d2f9779442432e0c58cfdafb9c7d7b1c8ec222ef79f09"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
